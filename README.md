# ğŸ“„ Vanilla RAG Document Analyzer

A **classic Retrieval-Augmented Generation (RAG)** system that allows users to upload PDF documents and ask **semantic questions** about their content using a Large Language Model.

---

## ğŸš€ Features

- Upload a PDF document via a Streamlit UI
- Split documents into semantic chunks
- Generate embeddings using a sentence-transformer model
- Store and retrieve document chunks using a vector database
- Answer user questions using LLM-based reasoning
- Grounded responses based strictly on document context
- Graceful fallback when information is not present

---

## â–¶ï¸ How to Run

### 1. Create and activate a virtual environment

python -m venv venv
source venv/bin/activate        # Linux / macOS
venv\Scripts\activate           # Windows

###2. Install dependencies

pip install -r requirements.txt

###3. Start the application

streamlit run main.py

âš  Note: Ensure Ollama is installed and a compatible LLaMA 3.x model is available locally.
The vector database (chroma_db/) is generated at runtime and is not included in the repo.

---

## ğŸ§  What this project demonstrates

This project focuses on **core RAG concepts**, including:

- Document ingestion and preprocessing
- Semantic vector search
- Context-aware answer generation
- Prompt grounding to reduce hallucinations
- Understanding the strengths and limitations of vanilla RAG

It is intentionally scoped to remain **simple, explainable, and production-aligned**.

---

## â— Limitations (by design)

This is a **vanilla RAG system**, which means:

- Exact string-based entities (e.g., email IDs, URLs) may not always be retrieved reliably
- PDF header content may be inconsistently extracted depending on formatting
- No deterministic field extraction or schema enforcement is applied
- No agent-based routing or tool usage is implemented

These limitations are **known characteristics of embedding-based semantic retrieval** and are intentionally left unhandled to preserve a pure RAG architecture.

---

## ğŸ›  Tech Stack

- **Python**
- **Streamlit** â€“ Web UI
- **LangChain** â€“ RAG orchestration
- **HuggingFace Sentence Transformers** â€“ Embeddings
- **Chroma** â€“ Vector database
- **Ollama (LLaMA 3.x)** â€“ Local LLM inference

---

## ğŸ“¦ Project Structure

```text
.
â”œâ”€â”€ main.py              # Streamlit application
â”œâ”€â”€ chroma_db/           # Persisted vector store
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # Project documentation


